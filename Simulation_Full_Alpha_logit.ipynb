{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy, sys\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load the data file to memeory.\n",
    "\n",
    "#Input: File path to read.\n",
    "#Output: A 2d numpy array with all loaded samples from the file to read in string.\n",
    "\n",
    "def parseFile_raw(file):\n",
    "    time_start = time.time()\n",
    "\n",
    "    content = []\n",
    "    count, count_incomplete,count_complete, count_part = 0, 0, 0, 0\n",
    "    \n",
    "    with open(file) as txtfile:\n",
    "        for row in txtfile:\n",
    "            \n",
    "            row = row.split(',')\n",
    "            row[-1] = row[-1].strip()\n",
    "            #if count != 0:\n",
    "            content.append([row[21]] + row[0:4] + [row[22]] + [row[32]] + row[24: 26] + [row[29]] + [row[6]] \\\n",
    "                           + [row[-5]] + [row[-4]] + [row[-3]] + [row[-2]] + [row[12].strip(\"'\")])\n",
    "\n",
    "            count += 1\n",
    "            #if count == 1000:\n",
    "                #break\n",
    "\n",
    "    content_mat = np.array(content)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('Reading data is complete! Running time is ' + str(time_end - time_start) + 's!')\n",
    "\n",
    "    return content_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load the data file to memeory, this is for the simulation hash data.\n",
    "\n",
    "#Input: File path to read.\n",
    "#Output: A 2d numpy array with all loaded samples from the file to read in string.\n",
    "\n",
    "def parseFile_reference(file):\n",
    "    time_start = time.time()\n",
    "\n",
    "    content = []\n",
    "    count, count_incomplete,count_complete, count_part = 0, 0, 0, 0\n",
    "    \n",
    "    with open(file) as txtfile:\n",
    "        for row in txtfile:\n",
    "            row = row.split(',')\n",
    "            row[-1] = row[-1].strip().strip(']').strip('\\n')\n",
    "            row[0] = row[0][1:]\n",
    "            row[0] = row[0].strip(\"'\")\n",
    "    \n",
    "            content.append(row)\n",
    "\n",
    "    reference_mat = np.array(content)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('Reading data is complete! Running time is ' + str(time_end - time_start) + 's!')\n",
    "\n",
    "    return reference_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFile_indi(file):\n",
    "\n",
    "    with open(file, 'r') as csvfile:\n",
    "        indi_list = []\n",
    "        for line in csvfile:\n",
    "            indi_list.append(line.strip().replace('-', ' ').split(','))\n",
    "\n",
    "    indicator_array = np.array(indi_list)\n",
    "    \n",
    "    return indicator_array\n",
    "\n",
    "def parseFile_hpi(file):\n",
    "\n",
    "    with open(file, 'r') as csvfile:\n",
    "        hpi_list = []\n",
    "        for line in csvfile:\n",
    "            hpi_list.append(line.strip().split(','))\n",
    "\n",
    "    hpi_array = np.array(hpi_list)\n",
    "    \n",
    "    return hpi_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter the samples with no missing values. \n",
    "#Input: mat - 2d Numpy Array.\n",
    "#Onput: mat - 2d Numpy Array with all samples that have no Missing values.\n",
    "\n",
    "def filter_full_feature(mat):\n",
    "    row_count = 0\n",
    "    full_list = []\n",
    "    for row in mat:\n",
    "        if 'N/A' in row or 'NA' in row:\n",
    "            pass\n",
    "        else:\n",
    "            full_list.append(row_count)\n",
    "\n",
    "        row_count += 1\n",
    "    print('There are a total of ' + str(len(full_list)) + ' samples fed into the model')\n",
    "    mat = mat[full_list, :]\n",
    "    return mat\n",
    "\n",
    "#Function to split the fullset into training and test sets.\n",
    "#Input: mat - 2d Numpy Array.\n",
    "#Onput: train_mat: 2d Numpy Array, test_mat: 2d Numpy Array\n",
    "def train_test_split(mat):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    num_sample, num_var = mat.shape\n",
    "\n",
    "    for i in range(0, num_sample):\n",
    "        if i == 0:\n",
    "            train_list.append(i)\n",
    "            test_list.append(i)\n",
    "        else:\n",
    "            rand = random.random()\n",
    "            if rand >= 0.1:\n",
    "                train_list.append(i)\n",
    "            else:\n",
    "                test_list.append(i)\n",
    "\n",
    "    train_mat = mat[train_list, :]\n",
    "    test_mat = mat[test_list, :]\n",
    "\n",
    "    return train_mat, test_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a probability into the coordinate of a zip code using population probability distritbuion\n",
    "#prob: float between 0 and 1\n",
    "#reference_array: a 2-d array contaning the coordinates of the reference zipcodes\n",
    "#prob_dist: a 1-d array shows the accumulated population distribution as a percentage of the total population in the US.\n",
    "def getzip(prob, reference_array, prob_dist):\n",
    "    idx = np.where(prob_dist >= prob)\n",
    "    idx = idx[0][0]\n",
    "    coord = reference_array[idx, :]\n",
    "    \n",
    "    return coord, idx\n",
    "\n",
    "#convert the index and probability from getzip() and get the gender of the simulation from a gender reference\n",
    "#idx: the index returned from getzip()\n",
    "#gender_ref: a 2-d Array that contains the gender distribution of each zip code.\n",
    "# 1-male, 0-female.\n",
    "def getgender(idx, gender_ref):\n",
    "    prob = random.random()\n",
    "    \n",
    "    if prob >= gender_ref[idx]:\n",
    "        gender = 0\n",
    "    else:\n",
    "        gender = 1\n",
    "    \n",
    "    return gender\n",
    "\n",
    "#convert the index, gender and a probability from getzip() and get the age of the simulation from an age reference\n",
    "def getage(idx, age_ref, gender):\n",
    "    age_ref_male = age_ref[:, :18]\n",
    "    age_ref_female = age_ref[:, 18:]\n",
    "    prob = random.random()\n",
    "    \n",
    "    if gender == 1:\n",
    "        idx_age = np.where(age_ref_male[idx] >= prob)\n",
    "        if idx_age[0].size != 0:\n",
    "            idx_age = idx_age[0][0]\n",
    "            delta = random.randint(0, 4)\n",
    "        \n",
    "            age = idx_age * 5 + delta\n",
    "        else:\n",
    "            age = 90\n",
    "    else:\n",
    "        idx_age = np.where(age_ref_female[idx] >= prob)\n",
    "        if idx_age[0].size != 0:\n",
    "            idx_age = idx_age[0][0]\n",
    "            delta = random.randint(0, 4)\n",
    "        \n",
    "            age = idx_age * 5 + delta\n",
    "        else:\n",
    "            age = 90\n",
    "\n",
    "    return age\n",
    "\n",
    "#convert the index and a probability from getzip() and get the race of the simulation from a race reference\n",
    "def getrace(idx, race_ref):\n",
    "    prob = random.random()\n",
    "    idx_race = np.where(race_ref[idx] >= prob)\n",
    "    idx_race = idx_race[0][0]\n",
    "    \n",
    "    return idx_race + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a zip code to its coresponding coordinate.\n",
    "#zip_array: a 1-d array that is a list of zip_code\n",
    "#reference_array: a 2-d array contaning the coordinates of the reference zipcodes\n",
    "def zip_to_coordinate(zip_array, reference_array):\n",
    "    count = 0\n",
    "    coordinate_list = []\n",
    "    full_list = []\n",
    "    zip_ref = reference_array[:, 0].astype(np.int)\n",
    "    for zip_c in zip_array:\n",
    "        idx = np.argwhere(zip_ref == int(zip_c))\n",
    "        if idx.size != 0:\n",
    "            coordinate_pair = reference_array[idx[0][0], 1:3]\n",
    "            full_list.append(count)\n",
    "        else: #there are some zipcodes were P.O box addresses and not in our reference. So we look for the nearby zipcodes\n",
    "            zip_c_back = int(zip_c) - 1\n",
    "            zip_c_forward = int(zip_c) + 1\n",
    "            idx_back = np.argwhere(zip_ref == zip_c_back)\n",
    "            idx_forward = np.argwhere(zip_ref == zip_c_forward)\n",
    "            if idx_back.size != 0:\n",
    "                coordinate_pair = reference_array[idx_back[0][0], 1:3]\n",
    "                full_list.append(count)\n",
    "            elif idx_forward.size != 0:\n",
    "                coordinate_pair = reference_array[idx_forward[0][0], 1:3]\n",
    "                full_list.append(count)\n",
    "            else:\n",
    "                coordinate_pair = ['N/A', 'N/A']\n",
    "                \n",
    "        count += 1\n",
    "        coordinate_list.append(coordinate_pair)\n",
    "    return np.array(coordinate_list), full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shuffle(array, upper_array, lower_array):\n",
    "    element_counter = 0\n",
    "    for element in array[0]:\n",
    "        prob = random.random()\n",
    "        if prob <= 0.15:\n",
    "            array[0][element_counter] = np.random.choice(np.arange(lower_array[element_counter], upper_array[element_counter]))  \n",
    "        element_counter += 1\n",
    "        \n",
    "    return array\n",
    "\n",
    "def convert_dummy(array):\n",
    "    \n",
    "    num_sample, num_feature = array.shape\n",
    "    dummy_list = []\n",
    "    \n",
    "    combined_df = np.array(pd.get_dummies(array[:, 0]))\n",
    "    for i in range(1, num_feature):\n",
    "        dummy_df = pd.get_dummies(array[:, i])\n",
    "        combined_df = np.concatenate((combined_df, np.array(dummy_df)), axis=1)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(neigh_model, zip_array ,coordinate_array, gender_array, age_array, race_array, prob_dist, daily_indicator, year, norm_model):\n",
    "    \n",
    "    # generate a random probability prop to population distri. (use zip for now)\n",
    "    prob = random.random() # 0.0~1.0\n",
    "    \n",
    "    #longi, lati = getcoord(zip)\n",
    "    coordinate, idx = getzip(prob, coordinate_array, prob_dist)\n",
    "    \n",
    "    #zip_c = int(zip_array[idx])\n",
    "    \n",
    "    gender = getgender(idx, gender_array)\n",
    "    \n",
    "    x_knn = np.append(coordinate, gender)\n",
    "    \n",
    "    age = getage(idx, age_array, gender)\n",
    "    \n",
    "    x_knn = np.append(x_knn, age)\n",
    "    \n",
    "    race = getrace(idx, race_array)\n",
    "    \n",
    "    x_knn = np.append(x_knn, race)\n",
    "    \n",
    "    x_knn = np.append(x_knn, daily_indicator)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    hpi_array = parseFile_hpi(\"CleanedData/hpi_cleaned.csv\")\n",
    "    hpi_locator = hpi_array[1:, 0:2].astype(np.int)\n",
    "    idx_hpi = np.argwhere(np.logical_and(hpi_locator[:,0] == zip_c, hpi_locator[:,1] == year))\n",
    "    \n",
    "    if idx_hpi.size != 0:\n",
    "        x_knn = np.append(x_knn, float(hpi_array[idx_hpi[0][0], 2])).reshape(1, 9)\n",
    "    else:\n",
    "        x_knn = np.append(x_knn, 0).reshape(1, 9)\n",
    "    \n",
    "    '''\n",
    "    x_knn_for_tran = copy.deepcopy(x_knn)\n",
    "    scaled_x_knn = norm_model.transform([x_knn_for_tran])\n",
    "    \n",
    "    # generate sentiment features (use knn for now)\n",
    "    senti_feature = neigh_model.predict(scaled_x_knn.reshape(1,8))\n",
    "    \n",
    "    upper_limit = [5, 2, 11, 11, 2, 4, 2, 2]\n",
    "    lower_limit = [1, -1, 0, 0, 0, 1, 0, 0]\n",
    "    \n",
    "    senti_feature = random_shuffle(senti_feature, upper_limit, lower_limit)\n",
    "    \n",
    "    return senti_feature, x_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(mat, label_location):\n",
    "    #model = linear_model.LogisticRegression()\n",
    "    poly = preprocessing.PolynomialFeatures(2)\n",
    "    num_sam, num_var = mat.shape\n",
    "    model = ensemble.RandomForestClassifier(n_estimators = 15,min_samples_split= 32, min_samples_leaf = 20)\n",
    "    feature_mat = np.delete(mat, label_location, axis=1).astype(np.float)\n",
    "    feature_mat = poly.fit_transform(feature_mat)\n",
    "    #feature_mat = np.concatenate((feature_mat, (feature_mat[:, 41] * feature_mat[:, 41]).reshape((num_sam, 1))), axis=1)\n",
    "    labels = mat[:, label_location].astype(np.int)\n",
    "    print('Model training - Started!')\n",
    "    time_start = time.time()\n",
    "    model.fit(feature_mat, labels)\n",
    "    time_end = time.time()\n",
    "    print('Model training - Completed! Training time: ' + str(time_end - time_start) + 's')\n",
    "\n",
    "    predicted_lab = model.predict(feature_mat)\n",
    "    corrected_pred = np.sum(labels == predicted_lab)\n",
    "\n",
    "    training_error = 1 - corrected_pred/labels.size\n",
    "\n",
    "    return model, training_error\n",
    "\n",
    "\n",
    "def model_test(model, mat, label_location):\n",
    "    num_sam, num_var = mat.shape\n",
    "    poly = preprocessing.PolynomialFeatures(2)\n",
    "    feature_mat = np.delete(mat, label_location, axis=1).astype(np.float)\n",
    "    feature_mat = poly.fit_transform(feature_mat)\n",
    "    #feature_mat = np.concatenate((feature_mat, (feature_mat[:, 41] * feature_mat[:, 41]).reshape((num_sam, 1))), axis=1)\n",
    "    labels = mat[:, label_location].astype(np.int)\n",
    "\n",
    "    predicted_lab = model.predict(feature_mat)\n",
    "    corrected_pred = np.sum(labels == predicted_lab)\n",
    "    \n",
    "    label_score = model.predict_proba(feature_mat)\n",
    "    \n",
    "    print('The current model stands an AUC of ' + str(roc_auc_score(labels, label_score[:, 1])))\n",
    "    \n",
    "    np.savetxt('predicted_lab_Logit.txt', predicted_lab.astype(np.int))\n",
    "    np.savetxt('label_test_Logit.txt', labels.astype(np.int))\n",
    "\n",
    "    test_error = 1 - corrected_pred / labels.size\n",
    "    return test_error\n",
    "\n",
    "def model_sim(model, mat):\n",
    "    poly = preprocessing.PolynomialFeatures(2)\n",
    "    feature_mat = mat.astype(np.float)\n",
    "    feature_mat = poly.fit_transform(feature_mat)\n",
    "    num_sim, num_var = feature_mat.shape\n",
    "    #feature_mat = np.concatenate((feature_mat, (feature_mat[:, 41] * feature_mat[:, 41]).reshape((num_sim, 1))), axis=1)\n",
    "    predicted_lab = model.predict(feature_mat).reshape(num_sim, 1)\n",
    "    \n",
    "    full_mat = np.concatenate((feature_mat, predicted_lab), axis=1)\n",
    "    \n",
    "    return full_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data is complete! Running time is 33.25624108314514s!\n",
      "There are a total of 857696 samples fed into the model\n",
      "Reading data is complete! Running time is 0.8344016075134277s!\n",
      "There are a total of 32800 samples fed into the model\n",
      "Reading data is complete! Running time is 0.1641831398010254s!\n",
      "There are a total of 32800 samples fed into the model\n",
      "normalization starts!\n",
      "normalization ends after 0.248549222946167\n",
      "KNN starts!\n",
      "KNN ends!\n",
      "Model training - Started!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file = \"CleanedData/gallup_clean_NA_determinant.txt\"\n",
    "    file_age = \"CleanedData/ppl_by_zip.txt\"\n",
    "    file_race = \"CleanedData/race_by_zip.txt\"\n",
    "    \n",
    "    file_indi = \"CleanedData/daily_ind_1.csv\"\n",
    "    file_indi_oil = \"CleanedData/daily_ind_oil.csv\"\n",
    "    file_indi_S = \"CleanedData/daily_ind_SP500.csv\"\n",
    "    \n",
    "    simu_iter = 100000        #327500000 is current US population\n",
    "    \n",
    "    raw_data = parseFile_raw(file) #raw_data from Gallup daily survey\n",
    "    header = raw_data[0,:]\n",
    "    cleaned_data_input = filter_full_feature(raw_data)[1:,:]  #cleaned_data input from Gallup\n",
    "    \n",
    "    label = cleaned_data_input[:, :1] #employmed label\n",
    "    cleaned_data = cleaned_data_input[:, 1:]\n",
    "    \n",
    "    age_data = parseFile_reference(file_age)\n",
    "    age_data = filter_full_feature(age_data)[1:,:]\n",
    "    coordinate = age_data[:,2:4].astype(np.float) # (longi,lati)\n",
    "    index = age_data[:,-1].astype(np.float) # prob\n",
    "    age_dist = age_data[:, 5:-2].astype(np.float)\n",
    "    \n",
    "    race_data = parseFile_reference(file_race)\n",
    "    race_data = filter_full_feature(race_data)[1:,:]\n",
    "    race_dist = race_data[:, 2:].astype(np.float)\n",
    "    \n",
    "    zipcode = age_data[:,:1]\n",
    "    gender_distribution = age_data[:, 4:5].astype(np.float)\n",
    "    \n",
    "    combined_zip_ref = np.concatenate((zipcode, coordinate), axis=1)\n",
    "    \n",
    "    zip_code_ind = cleaned_data[:, -1] \n",
    "    ind_coordinate, full_list = zip_to_coordinate(zip_code_ind, combined_zip_ref) # coreponding coordinate of the samples.\n",
    "    \n",
    "    content_mat = np.concatenate((cleaned_data,ind_coordinate),axis=1)\n",
    "    content_mat = content_mat[full_list, :]\n",
    "    label = label[full_list, :]\n",
    "    \n",
    "    num_sam, num_var = content_mat.shape\n",
    "    \n",
    "    X_knn = content_mat[:, -2:num_var + 1].astype(np.float)# zip vec\n",
    "    gender = content_mat[:, 8].astype(np.int)\n",
    "    age = content_mat[:, 9].astype(np.int)\n",
    "    race = content_mat[:, 10].astype(np.int)\n",
    "    daily_ind =  content_mat[:, 11:14].astype(np.float)\n",
    "    #hpi = content_mat[:, -4].astype(np.float)\n",
    "    #X_knn = np.concatenate((X_knn, gender.reshape((num_sam, 1)), age.reshape((num_sam, 1)), race.reshape((num_sam, 1)),daily_ind.reshape((num_sam, 3)), hpi.reshape(num_sam, 1) ),axis=1)\n",
    "    X_knn = np.concatenate((X_knn, gender.reshape((num_sam, 1)), age.reshape((num_sam, 1)), race.reshape((num_sam, 1)),daily_ind.reshape((num_sam, 3))),axis=1)\n",
    "    y_knn = content_mat[:,:8].astype(np.int) # senti mat\n",
    "    \n",
    "    X_for_transform = copy.deepcopy(X_knn)\n",
    "    \n",
    "    time_s = time.time()\n",
    "    print('normalization starts!')\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    scaler.fit(X_knn)\n",
    "    scaled_X_knn = scaler.transform(X_for_transform)\n",
    "    time_s_end = time.time()\n",
    "    print('normalization ends after ' + str(time_s_end - time_s))\n",
    "    \n",
    "    scaled_X_knn = np.around(scaled_X_knn, decimals = 10)\n",
    "    \n",
    "    sim_prob_ref = age_data[:, -1].astype(np.float)\n",
    "    \n",
    "    print('KNN starts!')\n",
    "    neigh = KNeighborsClassifier(n_neighbors=15, weights= 'distance')\n",
    "    neigh.fit(scaled_X_knn, y_knn)\n",
    "    print('KNN ends!')\n",
    "    \n",
    "    dummy_y = convert_dummy(y_knn)\n",
    "    \n",
    "    content_mat = np.concatenate((label, dummy_y, X_knn), axis=1)\n",
    "    train_mat, test_mat = train_test_split(content_mat)\n",
    "    \n",
    "    model, train_error = model_train(train_mat, 0)\n",
    "    test_error = model_test(model, test_mat, 0)\n",
    "    \n",
    "    print('The training error for this trail is: ' + str(train_error))\n",
    "    print('The testing error for this trail is: ' + str(test_error))\n",
    "    \n",
    "    indi_array = parseFile_indi(file_indi)\n",
    "    year_list = indi_array[:, 0]\n",
    "    indi_list = indi_array[:, 1].astype(np.float)\n",
    "    oil_array = parseFile_indi(file_indi_oil)\n",
    "    ind_oil_list = oil_array[:,1].astype(np.float)\n",
    "    S_array = parseFile_indi(file_indi_S)\n",
    "    ind_S_list = S_array[:,1].astype(np.float)\n",
    "    \n",
    "    num_sim_r = indi_list.shape\n",
    "    \n",
    "    employment_rate_16 = []\n",
    "    employment_rate_25_54 = []\n",
    "    \n",
    "    #employment_rate_18.append('>18')\n",
    "    #employment_rate_25_54.append('25 - 54')\n",
    "    count = 0\n",
    "    for index in range(0, num_sim_r[0]):\n",
    "        time_start = time.time()\n",
    "        print('Started simulation cycle ' + str(count))\n",
    "        \n",
    "        ind_list = [indi_list[index], ind_oil_list[index], ind_S_list[index]]\n",
    "        year = int(year_list[index][-2:])\n",
    "        X_classify = []\n",
    "        coord_list = []\n",
    "        for i in range(simu_iter):\n",
    "            #senti_fea, coord = simulation(neigh, scaler, coordinate, gender_distribution, age_dist, race_dist, sim_prob_ref, ind_list)\n",
    "            senti_fea, coord = simulation(neigh, zipcode, coordinate, gender_distribution, age_dist, race_dist, sim_prob_ref, ind_list, year, scaler)\n",
    "            coord_list.append(coord)\n",
    "            X_classify.append(senti_fea[0])\n",
    "          \n",
    "    # throw simulated data into the model, predict their unemplotment rate\n",
    "        X_classify = np.array(X_classify)\n",
    "        coord_list = np.array(coord_list)\n",
    "        \n",
    "        dummy_classified = convert_dummy(X_classify)\n",
    "        \n",
    "        output_array = np.concatenate((dummy_classified, coord_list), axis=1)\n",
    "        \n",
    "        sim_result = model_sim(model, output_array)\n",
    "        \n",
    "        age_array = output_array[:, 41].reshape((simu_iter, 1))\n",
    "        employment_array = sim_result[:, -1].reshape((simu_iter, 1))\n",
    "        \n",
    "        over_16_list = np.argwhere(age_array >= 16)\n",
    "        age_25_54_list = np.argwhere((age_array >= 25) & (age_array <= 54))\n",
    "        #over_18_population = over_18_list.shape\n",
    "        num_over_16, dim = over_16_list.shape\n",
    "        num_25_54, dim = age_25_54_list.shape\n",
    "        \n",
    "        employment_array_16 = employment_array[over_16_list[:,0]]\n",
    "        employment_array_25_54 = employment_array[age_25_54_list[:,0]]\n",
    "        \n",
    "        employment_rate_16.append(np.sum(employment_array_16)/num_over_16)\n",
    "        employment_rate_25_54.append(np.sum(employment_array_25_54)/num_25_54)\n",
    "        \n",
    "        time_end = time.time()\n",
    "        print('Simulation Cycle ' + str(count) + ' finished in ' + str(time_end - time_start) + 's!')\n",
    "        count += 1\n",
    "        \n",
    "        #if count == 10:\n",
    "            #break\n",
    "        \n",
    "    age_16 = np.array(employment_rate_16).reshape((count, 1))\n",
    "    age_25_54 = np.array(employment_rate_25_54).reshape((count, 1))\n",
    "    \n",
    "    #ind_list_out = ind_list.reshape(num_sim, 1)\n",
    "    \n",
    "    output_array = np.concatenate((age_16, age_25_54), axis=1)\n",
    "    np.savetxt('sim_out_daily_RF_Norm'+ str(simu_iter) + '.txt', output_array, delimiter=',', fmt='%1.4f,%1.4f')\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
