{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-cae8f1f5cd76>:22 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-cae8f1f5cd76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    294\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 296\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-cae8f1f5cd76>:22 "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy, sys\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from scipy import stats\n",
    "from numpy import linalg as LA\n",
    "import math\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load the data file to memeory.\n",
    "\n",
    "#Input: File path to read.\n",
    "#Output: A 2d numpy array with all loaded samples from the file to read in string.\n",
    "\n",
    "def parseFile_raw(file):\n",
    "    time_start = time.time()\n",
    "\n",
    "    content = []\n",
    "    count, count_incomplete,count_complete, count_part = 0, 0, 0, 0\n",
    "    \n",
    "    with open(file) as txtfile:\n",
    "        for row in txtfile:\n",
    "            \n",
    "            row = row.split(',')\n",
    "            row[-1] = row[-1].strip()\n",
    "            #if count != 0:\n",
    "            content.append([row[21]] + row[0:4] + [row[22]] + [row[32]] + row[24: 26] + [row[29]] + [row[6]] \\\n",
    "                           + [row[-5]] + [row[-4]] + [row[-3]] + [row[-2]] + [row[12].strip(\"'\")])\n",
    "\n",
    "            count += 1\n",
    "            #if count == 1000:\n",
    "                #break\n",
    "\n",
    "    content_mat = np.array(content)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('Reading data is complete! Running time is ' + str(time_end - time_start) + 's!')\n",
    "\n",
    "    return content_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load the data file to memeory, this is for the simulation hash data.\n",
    "\n",
    "#Input: File path to read.\n",
    "#Output: A 2d numpy array with all loaded samples from the file to read in string.\n",
    "\n",
    "def parseFile_reference(file):\n",
    "    time_start = time.time()\n",
    "\n",
    "    content = []\n",
    "    count, count_incomplete,count_complete, count_part = 0, 0, 0, 0\n",
    "    \n",
    "    with open(file) as txtfile:\n",
    "        for row in txtfile:\n",
    "            row = row.split(',')\n",
    "            row[-1] = row[-1].strip().strip(']').strip('\\n')\n",
    "            row[0] = row[0][1:]\n",
    "            row[0] = row[0].strip(\"'\")\n",
    "    \n",
    "            content.append(row)\n",
    "\n",
    "    reference_mat = np.array(content)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('Reading data is complete! Running time is ' + str(time_end - time_start) + 's!')\n",
    "\n",
    "    return reference_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFile_indi(file):\n",
    "\n",
    "    with open(file, 'r') as csvfile:\n",
    "        indi_list = []\n",
    "        for line in csvfile:\n",
    "            indi_list.append(line.strip().replace('-', ' ').split(','))\n",
    "\n",
    "    indicator_array = np.array(indi_list)\n",
    "    \n",
    "    return indicator_array\n",
    "\n",
    "def parseFile_hpi(file):\n",
    "\n",
    "    with open(file, 'r') as csvfile:\n",
    "        hpi_list = []\n",
    "        for line in csvfile:\n",
    "            hpi_list.append(line.strip().split(','))\n",
    "\n",
    "    hpi_array = np.array(hpi_list)\n",
    "    \n",
    "    return hpi_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter the samples with no missing values. \n",
    "#Input: mat - 2d Numpy Array.\n",
    "#Onput: mat - 2d Numpy Array with all samples that have no Missing values.\n",
    "\n",
    "def filter_full_feature(mat):\n",
    "    row_count = 0\n",
    "    full_list = []\n",
    "    for row in mat:\n",
    "        if 'N/A' in row or 'NA' in row:\n",
    "            pass\n",
    "        else:\n",
    "            full_list.append(row_count)\n",
    "\n",
    "        row_count += 1\n",
    "    print('There are a total of ' + str(len(full_list)) + ' samples fed into the model')\n",
    "    mat = mat[full_list, :]\n",
    "    return mat\n",
    "\n",
    "#Function to split the fullset into training and test sets.\n",
    "#Input: mat - 2d Numpy Array.\n",
    "#Onput: train_mat: 2d Numpy Array, test_mat: 2d Numpy Array\n",
    "def train_test_split(mat):\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    num_sample, num_var = mat.shape\n",
    "\n",
    "    for i in range(0, num_sample):\n",
    "        if i == 0:\n",
    "            train_list.append(i)\n",
    "            test_list.append(i)\n",
    "        else:\n",
    "            rand = random.random()\n",
    "            if rand >= 0.1:\n",
    "                train_list.append(i)\n",
    "            else:\n",
    "                test_list.append(i)\n",
    "\n",
    "    train_mat = mat[train_list, :]\n",
    "    test_mat = mat[test_list, :]\n",
    "\n",
    "    return train_mat, test_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a probability into the coordinate of a zip code using population probability distritbuion\n",
    "#prob: float between 0 and 1\n",
    "#reference_array: a 2-d array contaning the coordinates of the reference zipcodes\n",
    "#prob_dist: a 1-d array shows the accumulated population distribution as a percentage of the total population in the US.\n",
    "def getzip(prob, reference_array, prob_dist):\n",
    "    idx = np.where(prob_dist >= prob)\n",
    "    idx = idx[0][0]\n",
    "    coord = reference_array[idx, :]\n",
    "    \n",
    "    return coord, idx\n",
    "\n",
    "#convert the index and probability from getzip() and get the gender of the simulation from a gender reference\n",
    "#idx: the index returned from getzip()\n",
    "#gender_ref: a 2-d Array that contains the gender distribution of each zip code.\n",
    "# 1-male, 0-female.\n",
    "def getgender(idx, gender_ref):\n",
    "    prob = random.random()\n",
    "    \n",
    "    if prob >= gender_ref[idx]:\n",
    "        gender = 0\n",
    "    else:\n",
    "        gender = 1\n",
    "    \n",
    "    return gender\n",
    "\n",
    "#convert the index, gender and a probability from getzip() and get the age of the simulation from an age reference\n",
    "def getage(idx, age_ref, gender):\n",
    "    age_ref_male = age_ref[:, :18]\n",
    "    age_ref_female = age_ref[:, 18:]\n",
    "    prob = random.random()\n",
    "    \n",
    "    if gender == 1:\n",
    "        idx_age = np.where(age_ref_male[idx] >= prob)\n",
    "        if idx_age[0].size != 0:\n",
    "            idx_age = idx_age[0][0]\n",
    "            delta = random.randint(0, 4)\n",
    "        \n",
    "            age = idx_age * 5 + delta\n",
    "        else:\n",
    "            age = 90\n",
    "    else:\n",
    "        idx_age = np.where(age_ref_female[idx] >= prob)\n",
    "        if idx_age[0].size != 0:\n",
    "            idx_age = idx_age[0][0]\n",
    "            delta = random.randint(0, 4)\n",
    "        \n",
    "            age = idx_age * 5 + delta\n",
    "        else:\n",
    "            age = 90\n",
    "\n",
    "    return age\n",
    "\n",
    "#convert the index and a probability from getzip() and get the race of the simulation from a race reference\n",
    "def getrace(idx, race_ref):\n",
    "    prob = random.random()\n",
    "    idx_race = np.where(race_ref[idx] >= prob)\n",
    "    idx_race = idx_race[0][0]\n",
    "    \n",
    "    return idx_race + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert a zip code to its coresponding coordinate.\n",
    "#zip_array: a 1-d array that is a list of zip_code\n",
    "#reference_array: a 2-d array contaning the coordinates of the reference zipcodes\n",
    "def zip_to_coordinate(zip_array, reference_array):\n",
    "    count = 0\n",
    "    coordinate_list = []\n",
    "    full_list = []\n",
    "    zip_ref = reference_array[:, 0].astype(np.int)\n",
    "    for zip_c in zip_array:\n",
    "        idx = np.argwhere(zip_ref == int(zip_c))\n",
    "        if idx.size != 0:\n",
    "            coordinate_pair = reference_array[idx[0][0], 1:3]\n",
    "            full_list.append(count)\n",
    "        else: #there are some zipcodes were P.O box addresses and not in our reference. So we look for the nearby zipcodes\n",
    "            zip_c_back = int(zip_c) - 1\n",
    "            zip_c_forward = int(zip_c) + 1\n",
    "            idx_back = np.argwhere(zip_ref == zip_c_back)\n",
    "            idx_forward = np.argwhere(zip_ref == zip_c_forward)\n",
    "            if idx_back.size != 0:\n",
    "                coordinate_pair = reference_array[idx_back[0][0], 1:3]\n",
    "                full_list.append(count)\n",
    "            elif idx_forward.size != 0:\n",
    "                coordinate_pair = reference_array[idx_forward[0][0], 1:3]\n",
    "                full_list.append(count)\n",
    "            else:\n",
    "                coordinate_pair = ['N/A', 'N/A']\n",
    "                \n",
    "        count += 1\n",
    "        coordinate_list.append(coordinate_pair)\n",
    "    return np.array(coordinate_list), full_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shuffle(array, upper_array, lower_array):\n",
    "    element_counter = 0\n",
    "    for element in array[0]:\n",
    "        prob = random.random()\n",
    "        if prob <= 0.15:\n",
    "            array[0][element_counter] = np.random.choice(np.arange(lower_array[element_counter], upper_array[element_counter]))  \n",
    "        element_counter += 1\n",
    "        \n",
    "    return array\n",
    "\n",
    "def convert_dummy(array):\n",
    "    \n",
    "    num_sample, num_feature = array.shape\n",
    "    dummy_list = []\n",
    "    \n",
    "    combined_df = np.array(pd.get_dummies(array[:, 0]))\n",
    "    for i in range(1, num_feature):\n",
    "        dummy_df = pd.get_dummies(array[:, i])\n",
    "        combined_df = np.concatenate((combined_df, np.array(dummy_df)), axis=1)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cf(mat, sim_user, feat, k, mat_norm):\n",
    "#     #print(\"start cf\")\n",
    "#     mat += 0.0001\n",
    "#     #print(\"mat\")\n",
    "#     #print(mat[:2,:])\n",
    "#     sim_user += 0.0001\n",
    "#     sim_user = sim_user.reshape(-1,8) #(n,8)\n",
    "#     n = sim_user.shape[0]\n",
    "#     #print(\"sim_user before concat: \", sim_user)\n",
    "\n",
    "#     sim_user = np.concatenate((np.zeros((n,feat)), sim_user), axis=1)\n",
    "#     #print(\"sim_user after concat: \", sim_user)\n",
    "# #     cos_sim = sim_user.dot(mat.T) / LA.norm(sim_user) / LA.norm(mat,axis = 1)\n",
    "    \n",
    "#     sim_user = torch.tensor(sim_user,dtype=torch.float64).cuda()\n",
    "#     mat = torch.tensor(mat,dtype=torch.float64).cuda()\n",
    "#     mat_norm = torch.tensor(mat_norm,dtype=torch.float64).cuda()\n",
    "#     other = torch.tensor(0,dtype=torch.float64).cuda()\n",
    "#     sim_norm = torch.dist(sim_user, other)\n",
    "# #     nume = sim_user.dot(mat.T)\n",
    "# #     simu_norm = LA.norm(sim_user)\n",
    "# #     deno =  simu_norm * mat_norm\n",
    "#     print(\"shape\")\n",
    "#     print(sim_user.shape)\n",
    "#     print(mat.shape)\n",
    "#     print(sim_norm.shape)\n",
    "#     print(mat_norm.shape)\n",
    "#     cos_sim = torch.mm(sim_user, torch.t(mat)) / sim_norm / mat_norm\n",
    "# #     cos_sim = sim_user.dot(mat.T) / LA.norm(sim_user) / mat_norm\n",
    "#     print(cos_sim.shape)\n",
    "\n",
    "#     cos_sim = cos_sim.to(torch.device(\"cpu\")).numpy()\n",
    "#     mat = mat.to(torch.device(\"cpu\")).numpy()\n",
    "#     sim_user = sim_user.to(torch.device(\"cpu\")).numpy()\n",
    "#     idx = np.flip( np.argsort(cos_sim) , 1)[:,:k]\n",
    "#     tmp = np.flip( np.sort(cos_sim), 1)[:,:k]\n",
    "    \n",
    "# #     print(mat[idx,:])\n",
    "    \n",
    "#     for i in range(feat):\n",
    "#         #print(\"mat[idx,i]: \", mat[idx,i])\n",
    "# #         sim_user[:,i] = np.sum(mat[idx,i] * tmp, axis = 1)\n",
    "# #         sim_user[:,i] = np.sum(mat[idx,i], axis = 1) / k # avg\n",
    "        \n",
    "#         sim_user[:,i] = stats.mode(mat[idx,i], axis = 1)[0].T[0] # find most common value for each feature\n",
    "    \n",
    "#     #print(\"sim_user - 0.0001: \", sim_user - 0.0001)\n",
    "    \n",
    "# #     sim_user = round(sim_user - 0.0001)\n",
    "#     sim_user[:,:8] = (sim_user - 0.0001)[:,:8].astype(int)\n",
    "# #     sim_user = (sim_user - 0.001).astype(int)\n",
    "    \n",
    "#     return sim_user\n",
    "\n",
    "def cf(sim_user, mat, feat, k, mat_norm):\n",
    "# shapes (8,) and (16,856551) not aligned: 8 (dim 0) != 16 (dim 0)\n",
    "    sim_user = sim_user.reshape(1,16)\n",
    "    cos_sim = sim_user.dot(mat.T) / LA.norm(sim_user) / mat_norm\n",
    "    idx = np.flip( np.argsort(cos_sim).reshape(1,-1), 1)[:,:k]\n",
    "    tmp = np.flip( np.sort(cos_sim).reshape(1,-1), 1)[:,:k]\n",
    "    \n",
    "    sim_user[:8] = stats.mode(mat[idx,:][0], axis = 0)[0][0][:8]\n",
    "    # could not broadcast input array from shape (8) into shape (1,16)\n",
    "    return sim_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation(neigh_model, mat, zip_array ,coordinate_array, gender_array, age_array, race_array, prob_dist, daily_indicator, year, mat_norm):\n",
    "    \n",
    "    \n",
    "    # generate a random probability prop to population distri. (use zip for now)\n",
    "    prob = random.random() # 0.0~1.0\n",
    "    \n",
    "    #longi, lati = getcoord(zip)\n",
    "    coordinate, idx = getzip(prob, coordinate_array, prob_dist)\n",
    "    \n",
    "    #zip_c = int(zip_array[idx])\n",
    "    \n",
    "    gender = getgender(idx, gender_array)\n",
    "    \n",
    "    x_knn = np.append(coordinate, gender)\n",
    "    \n",
    "    age = getage(idx, age_array, gender)\n",
    "    \n",
    "    x_knn = np.append(x_knn, age)\n",
    "    \n",
    "    race = getrace(idx, race_array)\n",
    "    \n",
    "    x_knn = np.append(x_knn, race)\n",
    "    \n",
    "    x_knn = np.append(x_knn, daily_indicator)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    hpi_array = parseFile_hpi(\"CleanedData/hpi_cleaned.csv\")\n",
    "    hpi_locator = hpi_array[1:, 0:2].astype(np.int)\n",
    "    idx_hpi = np.argwhere(np.logical_and(hpi_locator[:,0] == zip_c, hpi_locator[:,1] == year))\n",
    "    \n",
    "    if idx_hpi.size != 0:\n",
    "        x_knn = np.append(x_knn, float(hpi_array[idx_hpi[0][0], 2])).reshape(1, 9)\n",
    "    else:\n",
    "        x_knn = np.append(x_knn, 0).reshape(1, 9)\n",
    "    \n",
    "    x_knn_for_tran = copy.deepcopy(x_knn)\n",
    "    #scaled_x_knn = norm_model.transform([x_knn_for_tran])\n",
    "    '''\n",
    "    # ------------------------------------------------\n",
    "    #print(\"start simulation\")\n",
    "    #print(\"x_knn: \", x_knn)\n",
    "    \n",
    "    # use cf to generate sentiment features\n",
    "    # 1. get mat\n",
    "    #print(\"get mat: \", mat.shape)\n",
    "    # 2. sim_user\n",
    "#     sim_user = np.array(x_knn).astype(float)\n",
    "#     feat, k = 8, 10\n",
    "    #print(\"sim_user before cf: \", sim_user)\n",
    "    # 3. apply cf\n",
    "#     sim_user = cf(mat, sim_user, feat, k, mat_norm)\n",
    "    #print(\"sim_user after cf: \", sim_user)\n",
    "#     senti_feature = sim_user[:,:8]\n",
    "    #print(\"senti_feature: \", senti_feature)\n",
    "#     #knn\n",
    "#     print(\"senti_feature: \", senti_feature)\n",
    "    \n",
    "# #     x_knn:  [ 3.4203289e+01 -9.7115520e+01  1.0000000e+00  2.8000000e+01\n",
    "# #               1.0000000e+00  2.4090000e+01  9.9640000e+01  1.4471600e+03]\n",
    "# #     senti_feature:  [[2 1 5 8 1 1 0 0]]\n",
    "    # ------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # generate sentiment features (use knn for now)\n",
    "#     senti_feature = neigh_model.predict(x_knn.reshape(1,8))\n",
    "    \n",
    "\n",
    "    \n",
    "#     upper_limit = [5, 2, 11, 11, 2, 4, 2, 2]\n",
    "#     lower_limit = [1, -1, 0, 0, 0, 1, 0, 0]\n",
    "    \n",
    "#     senti_feature = random_shuffle(senti_feature, upper_limit, lower_limit)\n",
    "    \n",
    "    \n",
    "#     return senti_feature, x_knn\n",
    "\n",
    "    return x_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(mat, label_location):\n",
    "    #model = linear_model.LogisticRegression()\n",
    "    poly = preprocessing.PolynomialFeatures(2)\n",
    "    num_sam, num_var = mat.shape\n",
    "    model = ensemble.RandomForestClassifier(n_estimators = 15,min_samples_split= 32, min_samples_leaf = 20)\n",
    "    feature_mat = np.delete(mat, label_location, axis=1).astype(np.float)\n",
    "    feature_mat = poly.fit_transform(feature_mat)\n",
    "    #feature_mat = np.concatenate((feature_mat, (feature_mat[:, 41] * feature_mat[:, 41]).reshape((num_sam, 1))), axis=1)\n",
    "    labels = mat[:, label_location].astype(np.int)\n",
    "    print('Model training - Started!')\n",
    "    time_start = time.time()\n",
    "    model.fit(feature_mat, labels)\n",
    "    time_end = time.time()\n",
    "    print('Model training - Completed! Training time: ' + str(time_end - time_start) + 's')\n",
    "\n",
    "    predicted_lab = model.predict(feature_mat)\n",
    "    corrected_pred = np.sum(labels == predicted_lab)\n",
    "\n",
    "    training_error = 1 - corrected_pred/labels.size\n",
    "\n",
    "    return model, training_error\n",
    "\n",
    "\n",
    "def model_test(model, mat, label_location):\n",
    "    num_sam, num_var = mat.shape\n",
    "    poly = preprocessing.PolynomialFeatures(2)\n",
    "    feature_mat = np.delete(mat, label_location, axis=1).astype(np.float)\n",
    "    feature_mat = poly.fit_transform(feature_mat)\n",
    "    #feature_mat = np.concatenate((feature_mat, (feature_mat[:, 41] * feature_mat[:, 41]).reshape((num_sam, 1))), axis=1)\n",
    "    labels = mat[:, label_location].astype(np.int)\n",
    "\n",
    "    predicted_lab = model.predict(feature_mat)\n",
    "    corrected_pred = np.sum(labels == predicted_lab)\n",
    "    \n",
    "    label_score = model.predict_proba(feature_mat)\n",
    "    \n",
    "    print('The current model stands an AUC of ' + str(roc_auc_score(labels, label_score[:, 1])))\n",
    "    \n",
    "    np.savetxt('predicted_lab_RF.txt', predicted_lab.astype(np.int))\n",
    "    np.savetxt('label_test_RF.txt', labels.astype(np.int))\n",
    "\n",
    "    test_error = 1 - corrected_pred / labels.size\n",
    "    return test_error\n",
    "\n",
    "def model_sim(model, mat):\n",
    "    poly = preprocessing.PolynomialFeatures(2)\n",
    "    feature_mat = mat.astype(np.float)\n",
    "    feature_mat = poly.fit_transform(feature_mat)\n",
    "    num_sim, num_var = feature_mat.shape\n",
    "    #feature_mat = np.concatenate((feature_mat, (feature_mat[:, 41] * feature_mat[:, 41]).reshape((num_sim, 1))), axis=1)\n",
    "    predicted_lab = model.predict(feature_mat).reshape(num_sim, 1)\n",
    "    \n",
    "    full_mat = np.concatenate((feature_mat, predicted_lab), axis=1)\n",
    "    \n",
    "    return full_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data is complete! Running time is 26.253950357437134s!\n",
      "There are a total of 857696 samples fed into the model\n",
      "Reading data is complete! Running time is 0.3672142028808594s!\n",
      "There are a total of 32800 samples fed into the model\n",
      "Reading data is complete! Running time is 0.07768750190734863s!\n",
      "There are a total of 32800 samples fed into the model\n",
      "Started simulation cycle 0\n",
      "before cf:  (10, 16)\n",
      "data.collect:  [array([ 1.0000000e-04,  1.0000000e-04,  1.0000000e-04,  1.0000000e-04,\n",
      "        1.0000000e-04,  1.0000000e-04,  1.0000000e-04,  1.0000000e-04,\n",
      "        3.0562114e+01, -9.6273800e+01,  1.0000000e-04,  2.1000100e+01,\n",
      "        1.0001000e+00,  2.4090100e+01,  9.9640100e+01,  1.4471601e+03])]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 51, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-38-aba5151af937>\", line 137, in <lambda>\n  File \"<ipython-input-35-184363b04ac4>\", line 62, in cf\nValueError: could not broadcast input array from shape (8) into shape (1,16)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-38-aba5151af937>\", line 137, in <lambda>\n  File \"<ipython-input-35-184363b04ac4>\", line 62, in cf\nValueError: could not broadcast input array from shape (8) into shape (1,16)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-aba5151af937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-aba5151af937>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.collect: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat_norm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.collect: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.collect: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.count: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1001\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1002\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 51, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-38-aba5151af937>\", line 137, in <lambda>\n  File \"<ipython-input-35-184363b04ac4>\", line 62, in cf\nValueError: could not broadcast input array from shape (8) into shape (1,16)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-38-aba5151af937>\", line 137, in <lambda>\n  File \"<ipython-input-35-184363b04ac4>\", line 62, in cf\nValueError: could not broadcast input array from shape (8) into shape (1,16)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$1.apply(PythonRDD.scala:141)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file = \"../CleanedData/gallup_clean_NA_determinant.txt\"\n",
    "    file_age = \"../CleanedData/ppl_by_zip.txt\"\n",
    "    file_race = \"../CleanedData/race_by_zip.txt\"\n",
    "    \n",
    "    file_indi = \"../CleanedData/daily_ind_1.csv\"\n",
    "    file_indi_oil = \"../CleanedData/daily_ind_oil.csv\"\n",
    "    file_indi_S = \"../CleanedData/daily_ind_SP500.csv\"\n",
    "    \n",
    "#     simu_iter = 10000        #327500000 is current US population\n",
    "    simu_iter = 10\n",
    "    \n",
    "    raw_data = parseFile_raw(file) #raw_data from Gallup daily survey\n",
    "    header = raw_data[0,:]\n",
    "    cleaned_data_input = filter_full_feature(raw_data)[1:,:]  #cleaned_data input from Gallup\n",
    "    \n",
    "    label = cleaned_data_input[:, :1] #employmed label\n",
    "    cleaned_data = cleaned_data_input[:, 1:]\n",
    "    \n",
    "    age_data = parseFile_reference(file_age)\n",
    "    age_data = filter_full_feature(age_data)[1:,:]\n",
    "    coordinate = age_data[:,2:4].astype(np.float) # (longi,lati)\n",
    "    index = age_data[:,-1].astype(np.float) # prob\n",
    "    age_dist = age_data[:, 5:-2].astype(np.float)\n",
    "    \n",
    "    race_data = parseFile_reference(file_race)\n",
    "    race_data = filter_full_feature(race_data)[1:,:]\n",
    "    race_dist = race_data[:, 2:].astype(np.float)\n",
    "    \n",
    "    zipcode = age_data[:,:1]\n",
    "    gender_distribution = age_data[:, 4:5].astype(np.float)\n",
    "    \n",
    "    combined_zip_ref = np.concatenate((zipcode, coordinate), axis=1)\n",
    "    \n",
    "    zip_code_ind = cleaned_data[:, -1] \n",
    "    ind_coordinate, full_list = zip_to_coordinate(zip_code_ind, combined_zip_ref) # coreponding coordinate of the samples.\n",
    "    \n",
    "    content_mat = np.concatenate((cleaned_data,ind_coordinate),axis=1)\n",
    "    content_mat = content_mat[full_list, :]\n",
    "    label = label[full_list, :]\n",
    "    \n",
    "    num_sam, num_var = content_mat.shape\n",
    "    \n",
    "    X_knn = content_mat[:, -2:num_var + 1].astype(np.float)# zip vec\n",
    "    gender = content_mat[:, 8].astype(np.int)\n",
    "    age = content_mat[:, 9].astype(np.int)\n",
    "    race = content_mat[:, 10].astype(np.int)\n",
    "    daily_ind =  content_mat[:, 11:14].astype(np.float)\n",
    "    #hpi = content_mat[:, -4].astype(np.float)\n",
    "    #X_knn = np.concatenate((X_knn, gender.reshape((num_sam, 1)), age.reshape((num_sam, 1)), race.reshape((num_sam, 1)),daily_ind.reshape((num_sam, 3)), hpi.reshape(num_sam, 1) ),axis=1)\n",
    "    X_knn = np.concatenate((X_knn, gender.reshape((num_sam, 1)), age.reshape((num_sam, 1)), race.reshape((num_sam, 1)),daily_ind.reshape((num_sam, 3))),axis=1)\n",
    "    y_knn = content_mat[:,:8].astype(np.int) # senti mat\n",
    "    \n",
    "#     print(\"get X_knn and y_knn: \")\n",
    "#     print(X_knn.shape) # (856551, 8)\n",
    "#     print(y_knn.shape) # (856551, 8)\n",
    "    mat = np.concatenate((y_knn,X_knn), axis=1)\n",
    "#     print(\"get mat: \", mat.shape)\n",
    "    '''\n",
    "    X_for_transform = copy.deepcopy(X_knn)\n",
    "    \n",
    "    time_s = time.time()\n",
    "    print('normalization starts!')\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    scaler.fit(X_knn)\n",
    "    scaled_X_knn = scaler.transform(X_for_transform)\n",
    "    time_s_end = time.time()\n",
    "    print('normalization ends after ' + str(time_s_end - time_s))\n",
    "    \n",
    "    scaled_X_knn = np.around(scaled_X_knn, decimals = 10)\n",
    "    '''\n",
    "    sim_prob_ref = age_data[:, -1].astype(np.float)\n",
    "    \n",
    "    neigh = None\n",
    "#     print('KNN starts!')\n",
    "#     neigh = KNeighborsClassifier(n_neighbors=10, weights= 'distance')\n",
    "#     neigh.fit(X_knn, y_knn)\n",
    "#     print('KNN ends!')\n",
    "    \n",
    "    \n",
    "    \n",
    "#     dummy_y = convert_dummy(y_knn)\n",
    "    \n",
    "#     content_mat = np.concatenate((label, dummy_y, X_knn), axis=1)\n",
    "#     train_mat, test_mat = train_test_split(content_mat)\n",
    "    \n",
    "#     model, train_error = model_train(train_mat, 0)\n",
    "#     test_error = model_test(model, test_mat, 0)\n",
    "    \n",
    "#     print('The training error for this trail is: ' + str(train_error))\n",
    "#     print('The testing error for this trail is: ' + str(test_error))\n",
    "    \n",
    "    indi_array = parseFile_indi(file_indi)\n",
    "    year_list = indi_array[:, 0]\n",
    "    indi_list = indi_array[:, 1].astype(np.float)\n",
    "    oil_array = parseFile_indi(file_indi_oil)\n",
    "    ind_oil_list = oil_array[:,1].astype(np.float)\n",
    "    S_array = parseFile_indi(file_indi_S)\n",
    "    ind_S_list = S_array[:,1].astype(np.float)\n",
    "    \n",
    "    num_sim_r = indi_list.shape\n",
    "    \n",
    "    employment_rate_16 = []\n",
    "    employment_rate_25_54 = []\n",
    "    \n",
    "    #employment_rate_18.append('>18')\n",
    "    #employment_rate_25_54.append('25 - 54')\n",
    "    count = 0\n",
    "    mat += 0.0001\n",
    "    mat_norm = LA.norm(mat,axis = 1)\n",
    "#     for index in range(0, num_sim_r[0]):\n",
    "    for index in range(10):\n",
    "        time_start = time.time()\n",
    "        print('Started simulation cycle ' + str(count))\n",
    "        \n",
    "        ind_list = [indi_list[index], ind_oil_list[index], ind_S_list[index]]\n",
    "        year = int(year_list[index][-2:])\n",
    "        X_classify = []\n",
    "        coord_list = []\n",
    "        sim_user = []\n",
    "        for i in range(simu_iter):\n",
    "            #senti_fea, coord = simulation(neigh, scaler, coordinate, gender_distribution, age_dist, race_dist, sim_prob_ref, ind_list)\n",
    "#             senti_fea, coord = simulation(neigh, mat, zipcode, coordinate, gender_distribution, age_dist, race_dist, sim_prob_ref, ind_list, year, mat_norm)\n",
    "\n",
    "            single_sim_user = simulation(neigh, mat, zipcode, coordinate, gender_distribution, age_dist, race_dist, sim_prob_ref, ind_list, year, mat_norm)\n",
    "            sim_user.append(single_sim_user)\n",
    "#         coord_list.append(coord)\n",
    "#         X_classify.append(senti_fea[0])\n",
    "        \n",
    "        sim_user = np.array(sim_user).astype(float)\n",
    "        feat, k = 8, 10\n",
    "        sim_user = np.concatenate((np.zeros((simu_iter,8)), sim_user), axis=1)\n",
    "        sim_user += 0.0001\n",
    "        print(\"before cf: \", sim_user.shape) # (10000, 8)\n",
    "        data = sc.parallelize(sim_user)\n",
    "        print(\"data.collect: \", data.take(1))\n",
    "        data = data.map(lambda user: cf(user, mat, feat, k, mat_norm) )\n",
    "        print(\"data.collect: \", data.take(1))\n",
    "        print(\"data.collect: \", data.collect())\n",
    "        print(\"data.count: \", data.count())\n",
    "        \n",
    "#         mat -= 0.0001\n",
    "        sim_user = np.array(data.collect())\n",
    "        sim_user -= 0.0001\n",
    "        \n",
    "#         sim_user = cf(mat, sim_user, feat, k, mat_norm)\n",
    "        print(\"after cf: \", sim_user.shape)\n",
    "        \n",
    "        X_classify = sim_user[:,8:]\n",
    "        coord_list = sim_user[:,:7]\n",
    "        \n",
    "        print (\"throw simulated data into the model\")\n",
    "#     # throw simulated data into the model, predict their unemplotment rate\n",
    "        X_classify = np.array(X_classify)\n",
    "        coord_list = np.array(coord_list)\n",
    "        \n",
    "        #print(\"get all X_classify and coord_list\")\n",
    "        #print(X_classify.shape) # (2, 8)\n",
    "        #print(coord_list.shape) # (2, 8)\n",
    "        \n",
    "#         dummy_classified = convert_dummy(X_classify)\n",
    "        \n",
    "#         output_array = np.concatenate((dummy_classified, coord_list), axis=1)\n",
    "#         print(\"get output_array: \", output_array.shape) # (2, 17)\n",
    "        \n",
    "#         sim_result = model_sim(model, output_array)\n",
    "        \n",
    "#         age_array = output_array[:, 41].reshape((simu_iter, 1))\n",
    "#         print(\"age_array: \", age_array)\n",
    "#         employment_array = sim_result[:, -1].reshape((simu_iter, 1))\n",
    "        \n",
    "#         over_16_list = np.argwhere(age_array >= 16)\n",
    "#         age_25_54_list = np.argwhere((age_array >= 25) & (age_array <= 54))\n",
    "#         #over_18_population = over_18_list.shape\n",
    "#         num_over_16, dim = over_16_list.shape\n",
    "#         num_25_54, dim = age_25_54_list.shape\n",
    "        \n",
    "#         employment_array_16 = employment_array[over_16_list[:,0]]\n",
    "#         employment_array_25_54 = employment_array[age_25_54_list[:,0]]\n",
    "        \n",
    "#         employment_rate_16.append(np.sum(employment_array_16)/num_over_16)\n",
    "#         employment_rate_25_54.append(np.sum(employment_array_25_54)/num_25_54)\n",
    "        \n",
    "        time_end = time.time()\n",
    "        print('Simulation Cycle ' + str(count) + ' finished in ' + str(time_end - time_start) + 's!')\n",
    "        count += 1\n",
    "        \n",
    "#         #if count == 10:\n",
    "#             #break\n",
    "        \n",
    "#     age_16 = np.array(employment_rate_16).reshape((count, 1))\n",
    "#     age_25_54 = np.array(employment_rate_25_54).reshape((count, 1))\n",
    "    \n",
    "#     #ind_list_out = ind_list.reshape(num_sim, 1)\n",
    "    \n",
    "#     output_array = np.concatenate((age_16, age_25_54), axis=1)\n",
    "#     np.savetxt('sim_out_daily_RF_CF_optim'+ str(simu_iter) + '.txt', output_array, delimiter=',', fmt='%1.4f,%1.4f')\n",
    "    \n",
    "    print(\"all simulation done !\")\n",
    "    \n",
    "if __name__ == \"__main__\":    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_user = np.array([[1,2,3],[4,5,6]]) # (2,3)\n",
    "mat = np.array([[1,2,3],[4,5,6],[7,8,9],[10,11,12]]) # (4,3)\n",
    "mat_norm = LA.norm(mat,axis = 1)\n",
    "\n",
    "sim_user = torch.tensor(sim_user,dtype=torch.float64).cuda()\n",
    "mat = torch.tensor(mat,dtype=torch.float64).cuda()\n",
    "mat_norm = torch.tensor(mat_norm,dtype=torch.float64).cuda()\n",
    "other = torch.tensor(0,dtype=torch.float64).cuda()\n",
    "\n",
    "sim_norm = torch.dist(sim_user, other)\n",
    "\n",
    "print(\"sim_user: \", sim_user)\n",
    "mat_t = torch.t(mat)\n",
    "print(\"mat_t: \",mat_t)\n",
    "cos_sim = torch.mm(sim_user, mat_t) \n",
    "print(\"cos_sim: \",cos_sim)\n",
    "print(\"sim_norm: \", sim_norm)\n",
    "cos_sim /= sim_norm\n",
    "print(\"cos_sim: \",cos_sim)\n",
    "print(\"mat_norm: \", mat_norm)\n",
    "cos_sim /= mat_norm\n",
    "print(\"cos_sim: \",cos_sim)\n",
    "\n",
    "print(cos_sim)\n",
    "cos_sim = cos_sim.to(torch.device(\"cpu\")).numpy()\n",
    "print(cos_sim)\n",
    "# torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.0718 / 8.7750\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-cae8f1f5cd76>:22 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3a4223af9548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msim_user\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_user\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(sim_user.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    294\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 296\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-cae8f1f5cd76>:22 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "sim_user = np.ones((10000, 8))\n",
    "print(sim_user[0,:])\n",
    "# print(sim_user.shape)\n",
    "\n",
    "data = sc.parallelize(sim_user)\n",
    "data.take(1)\n",
    "\n",
    "\n",
    "# # Load the data\n",
    "# data = sc.textFile('/home/shared/CF/pg100.txt')\n",
    "\n",
    "# # Parse the data into words\n",
    "# words = data.flatMap(lambda line: re.split(r'[^\\w]+', line))\n",
    "\n",
    "# # Deal with case\n",
    "# caps = words.map(lambda word: word.upper())\n",
    "\n",
    "# # Remove empty words\n",
    "# final = caps.filter(lambda word: word != '')\n",
    "\n",
    "# # Convert each word into a tuple of its first letter and 1\n",
    "# pairs = final.map(lambda word: (word[0], 1))\n",
    "\n",
    "# # Sum the counts for each character and save them to disk\n",
    "# pairs.reduceByKey(lambda c1, c2: c1 + c2).sortByKey().saveAsTextFile('/home/shared/CF/output.txt')\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf(sim_user, mat, feat, k, mat_norm):\n",
    "    \n",
    "    cos_sim = sim_user.dot(mat.T) / LA.norm(sim_user) / mat_norm\n",
    "    idx = np.flip( np.argsort(cos_sim) , 1)[:,:k]\n",
    "    tmp = np.flip( np.sort(cos_sim), 1)[:,:k]\n",
    "    \n",
    "    for i in range(feat):\n",
    "        sim_user[:,i] = stats.mode(mat[idx,i], axis = 1)[0].T[0]\n",
    "    \n",
    "    sim_user[:,:8] = sim_user[:,:8].astype(int)\n",
    "    \n",
    "    return sim_user\n",
    "\n",
    "def cf1(mat, sim_user, feat, k, mat_norm):\n",
    "    mat += 0.0001\n",
    "    sim_user += 0.0001\n",
    "    sim_user = sim_user.reshape(-1,8) #(n,8)\n",
    "\n",
    "    sim_user = np.concatenate((np.zeros((n,feat)), sim_user), axis=1)\n",
    "    cos_sim = sim_user.dot(mat.T) / LA.norm(sim_user) / LA.norm(mat)\n",
    "\n",
    "    idx = np.flip( np.argsort(cos_sim) , 1)[:,:k]\n",
    "    tmp = np.flip( np.sort(cos_sim), 1)[:,:k]\n",
    "    \n",
    "    for i in range(feat):\n",
    "#         sim_user[:,i] = np.sum(mat[idx,i] * tmp, axis = 1)\n",
    "#         sim_user[:,i] = np.sum(mat[idx,i], axis = 1) / k # avg\n",
    "        sim_user[:,i] = stats.mode(mat[idx,i], axis = 1)[0].T[0] # find most common value for each feature\n",
    "\n",
    "    sim_user[:,:8] = (sim_user - 0.0001)[:,:8].astype(int)\n",
    "    \n",
    "    return sim_user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 16)\n",
      "test\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 1 times, most recent failure: Lost task 2.0 in stage 4.0 (TID 27, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 206, in flip\n    indexer[axis] = slice(None, None, -1)\nIndexError: list assignment index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 362, in func\n    return f(iterator)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1056, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1056, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-20-fa642c147da8>\", line 28, in <lambda>\n  File \"<ipython-input-20-fa642c147da8>\", line 4, in cf\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 209, in flip\n    % (axis, m.ndim))\nValueError: axis=1 is invalid for the 1-dimensional input array\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 206, in flip\n    indexer[axis] = slice(None, None, -1)\nIndexError: list assignment index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 362, in func\n    return f(iterator)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1056, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1056, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-20-fa642c147da8>\", line 28, in <lambda>\n  File \"<ipython-input-20-fa642c147da8>\", line 4, in cf\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 209, in flip\n    % (axis, m.ndim))\nValueError: axis=1 is invalid for the 1-dimensional input array\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fa642c147da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmat\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# np.all(rdd.first() == mat[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \"\"\"\n\u001b[0;32m-> 1056\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \"\"\"\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \"\"\"\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/shared/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 1 times, most recent failure: Lost task 2.0 in stage 4.0 (TID 27, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 206, in flip\n    indexer[axis] = slice(None, None, -1)\nIndexError: list assignment index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 362, in func\n    return f(iterator)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1056, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1056, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-20-fa642c147da8>\", line 28, in <lambda>\n  File \"<ipython-input-20-fa642c147da8>\", line 4, in cf\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 209, in flip\n    % (axis, m.ndim))\nValueError: axis=1 is invalid for the 1-dimensional input array\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 206, in flip\n    indexer[axis] = slice(None, None, -1)\nIndexError: list assignment index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2438, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 362, in func\n    return f(iterator)\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1056, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1056, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"<ipython-input-20-fa642c147da8>\", line 28, in <lambda>\n  File \"<ipython-input-20-fa642c147da8>\", line 4, in cf\n  File \"/home/shared/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 209, in flip\n    % (axis, m.ndim))\nValueError: axis=1 is invalid for the 1-dimensional input array\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "def cf(sim_user, mat, feat, k, mat_norm):\n",
    "    \n",
    "    cos_sim = sim_user.dot(mat.T) / LA.norm(sim_user) / mat_norm\n",
    "    idx = np.flip( np.argsort(cos_sim) , 1)[:,:k]\n",
    "    tmp = np.flip( np.sort(cos_sim), 1)[:,:k]\n",
    "    \n",
    "    for i in range(feat):\n",
    "        sim_user[:,i] = stats.mode(mat[idx,i], axis = 1)[0].T[0]\n",
    "    \n",
    "    sim_user[:,:8] = sim_user[:,:8].astype(int)\n",
    "    \n",
    "    return sim_user\n",
    "\n",
    "# sc = SparkContext(conf=conf)\n",
    "mat = np.ones((856551, 16)) * 2\n",
    "sim_user = np.ones((100, 8))\n",
    "sim_user = np.concatenate((np.zeros((100,8)), sim_user), axis=1)\n",
    "print(sim_user.shape)\n",
    "data = sc.parallelize(sim_user)\n",
    "# data.take(2)\n",
    "# data.collect()\n",
    "\n",
    "feat, k = 8, 10\n",
    "mat += 0.0001\n",
    "sim_user += 0.0001\n",
    "mat_norm = LA.norm(mat)\n",
    "data = data.map(lambda user: cf(user, mat, feat, k, mat_norm) )\n",
    "# sim_user -= 0.0001\n",
    "mat -= 0.0001\n",
    "\n",
    "print(data.count())\n",
    "\n",
    "# np.all(rdd.first() == mat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
